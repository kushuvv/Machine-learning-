import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
# Load dataset
data = pd.read_csv("/content/diabetes.csv")
# Separate features and target
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values
# Normalize features
scaler = StandardScaler()
X = scaler.fit_transform(X)
# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Convert target to one-hot encoding (binary classification)
y_train = y_train.reshape(-1, 1)
y_test = y_test.reshape(-1, 1)
# ANN Implementation (Backpropagation)
class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, epochs=5000):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        self.epochs = epochs
        
        # Initialize weights
        self.W1 = np.random.randn(self.input_size, self.hidden_size)
        self.b1 = np.zeros((1, self.hidden_size))
        self.W2 = np.random.randn(self.hidden_size, self.output_size)
        self.b2 = np.zeros((1, self.output_size))

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def sigmoid_derivative(self, z):
        return z * (1 â€“ z)
    def train(self, X, y):
        for epoch in range(self.epochs):
            # Forward pass
            z1 = np.dot(X, self.W1) + self.b1
            a1 = self.sigmoid(z1)
            z2 = np.dot(a1, self.W2) + self.b2
            a2 = self.sigmoid(z2)
            # Compute error
            error = y - a2
            # Backpropagation
            d_a2 = error * self.sigmoid_derivative(a2)
            d_W2 = np.dot(a1.T, d_a2)
            d_b2 = np.sum(d_a2, axis=0, keepdims=True)
            d_a1 = np.dot(d_a2, self.W2.T) * self.sigmoid_derivative(a1)
            d_W1 = np.dot(X.T, d_a1)
            d_b1 = np.sum(d_a1, axis=0, keepdims=True)
            
            # Update weights
            self.W1 += self.learning_rate * d_W1
            self.b1 += self.learning_rate * d_b1
            self.W2 += self.learning_rate * d_W2
            self.b2 += self.learning_rate * d_b2
            # Print loss occasionally
            if epoch % 1000 == 0:
                loss = np.mean(np.square(error))
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
    def predict(self, X):
        z1 = np.dot(X, self.W1) + self.b1
        a1 = self.sigmoid(z1)
        z2 = np.dot(a1, self.W2) + self.b2
        a2 = self.sigmoid(z2)
        return (a2 > 0.5).astype(int)
# Train and Test ANN
# Define network: input = 8 features, hidden = 10 neurons, output = 1 (binary classification)
nn = NeuralNetwork(input_size=X_train.shape[1], hidden_size=10, output_size=1, learning_rate=0.01, epochs=5000)
# Train
nn.train(X_train, y_train)
# Test
y_pred = nn.predict(X_test)
print(f"\nTest Accuracy: {accuracy * 100:.2f}%")
